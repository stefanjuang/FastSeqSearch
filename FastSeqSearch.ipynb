{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "48XIEhqXL77f"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "The approach leverages the strengths of both FAISS and fastdtw to efficiently handle the search and refinement process:\n",
        "\n",
        "FAISS: Efficiently retrieves a large initial candidate pool based on the last step of the query sequence. This is fast and handles high-dimensional data well.\n",
        "fastdtw: Iteratively refines the candidate pool by considering increasingly longer subsequences, making it feasible to handle long sequences without getting bogged down by the complexity of full DTW computations for all candidates.\n",
        "This combination ensures that we maintain efficiency while still getting accurate results.\n",
        "\n",
        "Summary of the Approach\n",
        "Initial Candidate Retrieval: Use FAISS to quickly retrieve an initial set of candidates based on the last step of the query sequence.\n",
        "Iterative Refinement: Use fastdtw to iteratively refine the candidate pool. Start with the last step of the query sequence and progressively consider longer subsequences.\n",
        "At each step, compute the DTW distance for the current subsequence with the candidates.\n",
        "Sort the candidates based on the DTW distance and remove the bottom 10%.\n",
        "Continue until the candidate set is sufficiently small.\n",
        "Final Selection: Once the candidate pool is sufficiently small, compute the full DTW distance for the remaining candidates and select the one with the smallest distance.\n",
        "'''\n",
        "\n",
        "import numpy as np\n",
        "from fastdtw import fastdtw\n",
        "from scipy.spatial.distance import euclidean\n",
        "import faiss\n",
        "\n",
        "def create_index(vectors):\n",
        "    d = vectors.shape[1]  # dimension of the vectors\n",
        "    index = faiss.IndexFlatL2(d)  # L2 distance index\n",
        "    index.add(vectors)\n",
        "    return index\n",
        "\n",
        "def faiss_search(index, query, k=1000):\n",
        "    D, I = index.search(query, k)\n",
        "    return I, D\n",
        "\n",
        "def fastdtw_distance(seq1, seq2):\n",
        "    distance, _ = fastdtw(seq1, seq2, dist=euclidean)\n",
        "    return distance\n",
        "\n",
        "def fastseqsearch(context_length, sequences, query_sequence, feature_dim):\n",
        "    if not sequences:\n",
        "        print(\"Dataset is empty.\")\n",
        "        return None\n",
        "\n",
        "    flattened_sequences = np.vstack(sequences)  # Flatten the sequences for FAISS indexing\n",
        "    index = create_index(flattened_sequences)\n",
        "\n",
        "    # Step 1: Use the last step of the query sequence to find initial candidates\n",
        "    last_step_query = query_sequence[-1].reshape(1, -1)\n",
        "    initial_candidates, _ = faiss_search(index, last_step_query, k=1000)  # Start with 1000 candidates\n",
        "\n",
        "    # Convert initial candidates to a set for efficient refinement\n",
        "    candidate_set = set(initial_candidates.flatten() // context_length)  # Map back to sequence indices\n",
        "    print(f\"Initial candidate set size: {len(candidate_set)}\")\n",
        "\n",
        "    # Step 2: Iteratively refine the candidate list\n",
        "    for i in range(2, len(query_sequence) + 1):\n",
        "        subseq_query = query_sequence[-i:].reshape(-1, feature_dim)\n",
        "        distances = []\n",
        "        print(f\"Subsequence length: {i}, Candidate set size: {len(candidate_set)}\")\n",
        "\n",
        "        for candidate_idx in candidate_set:\n",
        "            try:\n",
        "                candidate_seq = sequences[candidate_idx].reshape(context_length, feature_dim)\n",
        "                distance = fastdtw_distance(subseq_query, candidate_seq[-i:])\n",
        "                distances.append((candidate_idx, distance))\n",
        "            except IndexError as e:\n",
        "                print(f\"IndexError for candidate_idx: {candidate_idx}, Max index: {len(sequences)-1}\")\n",
        "                continue\n",
        "\n",
        "        # Sort distances and remove the bottom 10%\n",
        "        distances.sort(key=lambda x: x[1])\n",
        "        cutoff_index = max(1, len(distances) - len(distances) // 10)\n",
        "        candidate_set = set([idx for idx, dist in distances[:cutoff_index]])\n",
        "        print(f\"New candidate set size after refinement: {len(candidate_set)}\")\n",
        "\n",
        "        if len(candidate_set) == 1:\n",
        "            break\n",
        "\n",
        "    # Step 3: Find the most similar sequence in the refined candidate list\n",
        "    highest_match = None\n",
        "    highest_score = float('inf')\n",
        "\n",
        "    for candidate_idx in candidate_set:\n",
        "        try:\n",
        "            candidate_seq = sequences[candidate_idx].reshape(context_length, feature_dim)\n",
        "            distance = fastdtw_distance(query_sequence, candidate_seq)\n",
        "            if distance < highest_score:\n",
        "                highest_score = distance\n",
        "                highest_match = candidate_idx\n",
        "        except IndexError as e:\n",
        "            print(f\"IndexError for candidate_idx: {candidate_idx}, Max index: {len(sequences)-1}\")\n",
        "            continue\n",
        "\n",
        "    print(f\"The most similar sequence is at index {highest_match} with a DTW distance of {highest_score}.\")\n",
        "    return highest_match\n",
        "\n",
        "# Example usage\n",
        "context_length = 10\n",
        "sample = 1000\n",
        "feature_dim = 256\n",
        "\n",
        "# Generate Sample Data\n",
        "sequences = [np.random.rand(context_length, feature_dim) for _ in range(sample)]\n",
        "query_sequence = np.random.rand(context_length, feature_dim)  # Example query sequence\n",
        "\n",
        "# Run the search\n",
        "most_similar_index = fastseqsearch(context_length, sequences, query_sequence, feature_dim)\n",
        "print(f\"Most similar sequence index: {most_similar_index}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from fastdtw import fastdtw\n",
        "from scipy.spatial.distance import euclidean\n",
        "import faiss\n",
        "\n",
        "def create_index(vectors):\n",
        "    \"\"\"\n",
        "    Creates a FAISS index from a given set of vectors.\n",
        "    Args:\n",
        "        vectors (numpy.ndarray): A numpy array of shape (nb, d) where 'nb' is the number of base vectors and 'd' is the dimension.\n",
        "    Returns:\n",
        "        faiss.IndexFlatL2: The created FAISS index.\n",
        "    \"\"\"\n",
        "    d = vectors.shape[1]  # dimension of the vectors\n",
        "    index = faiss.IndexFlatL2(d)  # L2 distance index\n",
        "    index.add(vectors)\n",
        "    return index\n",
        "\n",
        "def faiss_search(index, query, k=1000):\n",
        "    \"\"\"\n",
        "    Search for k nearest neighbors in the FAISS index.\n",
        "    Args:\n",
        "        index (faiss.IndexFlatL2): The FAISS index.\n",
        "        query (numpy.ndarray): The query vector.\n",
        "        k (int): Number of nearest neighbors to return.\n",
        "    Returns:\n",
        "        np.ndarray: Indices of the nearest neighbors.\n",
        "        np.ndarray: Distances to the nearest neighbors.\n",
        "    \"\"\"\n",
        "    D, I = index.search(query, k)\n",
        "    return I, D\n",
        "\n",
        "def fastdtw_distance(seq1, seq2):\n",
        "    \"\"\"\n",
        "    Computes the DTW distance between two sequences using the fastdtw algorithm.\n",
        "    Args:\n",
        "        seq1 (numpy.ndarray): The first sequence.\n",
        "        seq2 (numpy.ndarray): The second sequence.\n",
        "    Returns:\n",
        "        float: The DTW distance between the sequences.\n",
        "    \"\"\"\n",
        "    distance, _ = fastdtw(seq1, seq2, dist=euclidean)\n",
        "    return distance\n",
        "\n",
        "def test_fastseqsearch(context_length, feature_dim, sequences, query_sequence, expected_result):\n",
        "    \"\"\"\n",
        "    Test the FastSeqSearch algorithm with given sequences and a query sequence.\n",
        "    Args:\n",
        "        context_length (int): The length of each sequence.\n",
        "        feature_dim (int): The dimensionality of each feature vector.\n",
        "        sequences (list of numpy.ndarray): The list of sequences.\n",
        "        query_sequence (numpy.ndarray): The query sequence.\n",
        "        expected_result (int or None): The expected result index.\n",
        "    \"\"\"\n",
        "    if not sequences:\n",
        "        print(\"Dataset is empty.\")\n",
        "        assert expected_result is None, f\"Test failed: expected {expected_result}, got None\"\n",
        "        return\n",
        "\n",
        "    # Flatten the sequences for FAISS indexing\n",
        "    flattened_sequences = np.vstack(sequences)\n",
        "    index = create_index(flattened_sequences)\n",
        "\n",
        "    # Use the last step of the query sequence to find initial candidates\n",
        "    last_step_query = query_sequence[-1].reshape(1, -1)\n",
        "    initial_candidates, _ = faiss_search(index, last_step_query, k=1000)\n",
        "\n",
        "    # Map back to sequence indices\n",
        "    candidate_set = set(initial_candidates.flatten() // context_length)\n",
        "    print(f\"Initial candidate set size: {len(candidate_set)}\")\n",
        "\n",
        "    # Iteratively refine the candidate list\n",
        "    for i in range(2, len(query_sequence) + 1):\n",
        "        subseq_query = query_sequence[-i:].reshape(-1, feature_dim)\n",
        "        distances = []\n",
        "        print(f\"Subsequence length: {i}, Candidate set size: {len(candidate_set)}\")\n",
        "\n",
        "        for candidate_idx in candidate_set:\n",
        "            try:\n",
        "                candidate_seq = sequences[candidate_idx].reshape(context_length, feature_dim)\n",
        "                distance = fastdtw_distance(subseq_query, candidate_seq[-i:])\n",
        "                distances.append((candidate_idx, distance))\n",
        "            except IndexError:\n",
        "                continue\n",
        "\n",
        "        # Sort distances and remove the bottom 10%\n",
        "        distances.sort(key=lambda x: x[1])\n",
        "        cutoff_index = max(1, len(distances) - len(distances) // 10)\n",
        "        candidate_set = set([idx for idx, dist in distances[:cutoff_index]])\n",
        "        print(f\"New candidate set size after refinement: {len(candidate_set)}\")\n",
        "\n",
        "        if len(candidate_set) == 1:\n",
        "            break\n",
        "\n",
        "    # Find the most similar sequence in the refined candidate list\n",
        "    highest_match = None\n",
        "    highest_score = float('inf')\n",
        "\n",
        "    for candidate_idx in candidate_set:\n",
        "        try:\n",
        "            candidate_seq = sequences[candidate_idx].reshape(context_length, feature_dim)\n",
        "            distance = fastdtw_distance(query_sequence, candidate_seq)\n",
        "            if distance < highest_score:\n",
        "                highest_score = distance\n",
        "                highest_match = candidate_idx\n",
        "        except IndexError:\n",
        "            continue\n",
        "\n",
        "    print(f\"Expected: {expected_result}, Got: {highest_match}\")\n",
        "    assert highest_match == expected_result, f\"Test failed: expected {expected_result}, got {highest_match}\"\n",
        "\n",
        "# Fixed sequences for testing\n",
        "context_length = 10\n",
        "feature_dim = 256\n",
        "fixed_sequences = [\n",
        "    np.ones((context_length, feature_dim)),\n",
        "    np.zeros((context_length, feature_dim)),\n",
        "    np.full((context_length, feature_dim), 0.5),\n",
        "    np.random.rand(context_length, feature_dim),\n",
        "    np.random.rand(context_length, feature_dim)\n",
        "]\n",
        "\n",
        "# Test Cases\n",
        "print(\"Running Base Case Tests:\")\n",
        "test_fastseqsearch(context_length, feature_dim, fixed_sequences, np.full((context_length, feature_dim), 0.5), 2)  # Query with an existing sequence\n",
        "\n",
        "print(\"Running Edge Case Tests:\")\n",
        "# Empty dataset test\n",
        "try:\n",
        "    test_fastseqsearch(context_length, feature_dim, [], np.random.rand(context_length, feature_dim), None)\n",
        "except AssertionError as e:\n",
        "    print(e)\n",
        "\n",
        "# Single sequence in dataset\n",
        "test_fastseqsearch(context_length, feature_dim, [fixed_sequences[0]], fixed_sequences[0], 0)\n",
        "\n",
        "# Large dataset\n",
        "large_sequences = [np.random.rand(context_length, feature_dim) for _ in range(10000)]\n",
        "large_sequences[5000] = np.full((context_length, feature_dim), 0.5)\n",
        "test_fastseqsearch(context_length, feature_dim, large_sequences, np.full((context_length, feature_dim), 0.5), 5000)\n",
        "\n",
        "# Long sequence test\n",
        "long_sequences = [\n",
        "    np.random.rand(100, feature_dim),\n",
        "    np.full((100, feature_dim), 0.5),\n",
        "    np.random.rand(100, feature_dim),\n",
        "    np.random.rand(100, feature_dim),\n",
        "    np.random.rand(100, feature_dim)\n",
        "]\n",
        "test_fastseqsearch(100, feature_dim, long_sequences, long_sequences[1], 1)\n"
      ],
      "metadata": {
        "id": "BLhcslawUtjC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "u03IyG2kVbZL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}